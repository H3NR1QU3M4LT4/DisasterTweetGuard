{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2a0d2313-d2a6-4a2e-bc30-a95ba21d3cbb",
   "metadata": {},
   "source": [
    "# Read Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07ca3044-3740-4ba0-a72a-12aba458bd8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1533ffa5-b811-4930-b72e-3d20a70b2099",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_csv('../data/raw/train.csv')\n",
    "df_test = pd.read_csv('../data/raw/test.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94b8c0da-ab8a-4691-bbb4-4f9778339d78",
   "metadata": {},
   "source": [
    "# Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b0b3a4c-7941-4848-8584-2bc9f55bb29d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd02898c-76c5-4efe-811b-98c5a2784211",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Training Set Shape = {}'.format(df_train.shape))\n",
    "print('Training Set Memory Usage = {:.2f} MB'.format(df_train.memory_usage().sum() / 1024**2))\n",
    "print('Test Set Shape = {}'.format(df_test.shape))\n",
    "print('Test Set Memory Usage = {:.2f} MB'.format(df_test.memory_usage().sum() / 1024**2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05cef789-9ac8-4f72-94aa-e0cab9407e3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb7db4b4-46eb-4b4e-ba89-8334045e27c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac6a6525-41cd-455f-84f0-9135707f17af",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Number of unique values in keyword = {df_train[\"keyword\"].nunique()} (Training) - {df_test[\"keyword\"].nunique()} (Test)')\n",
    "print(f'Number of unique values in location = {df_train[\"location\"].nunique()} (Training) - {df_test[\"location\"].nunique()} (Test)')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "852a853c-89b7-41c8-8caa-82a207040c15",
   "metadata": {},
   "source": [
    "# Process to use Multple Pre trained models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cee820bd-ae34-489f-bc01-056f7fa7f2f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from datasets import load_dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3b23f32-da3d-4979-9555-d83c5d487bf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e0ca211-3cf0-45ef-bcd3-a854a3aa4c85",
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_to_drop = [\"keyword\", \"location\"]\n",
    "\n",
    "# Drop columns in df_train and reset index\n",
    "df_train.drop(columns=columns_to_drop, inplace=True)\n",
    "df_train.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# Drop columns in df_test and reset index\n",
    "df_test.drop(columns=columns_to_drop, inplace=True)\n",
    "df_test.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f25423b5-3445-45fa-955f-e1640004b714",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the 'disaster' column\n",
    "df_train['disaster'] = np.where(df_train['target'] == 1, True, False)\n",
    "\n",
    "# Create the 'no_disaster' column\n",
    "df_train['no_disaster'] = np.where(df_train['target'] == 0, True, False)\n",
    "\n",
    "# Drop the original 'target' column if needed\n",
    "df_train.drop(columns=['target'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "156cd43a-642c-4f16-a4db-18ef82f4bd9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train, df_val = train_test_split(df_train, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1716a4f-207b-4858-9d29-11fce9b7380b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_datasets(datasets, *filepaths):\n",
    "    \"\"\"Save pandas DataFrames to csv files.\"\"\"\n",
    "    for dataset, filepath in zip(datasets, filepaths):\n",
    "        dataset.to_csv(filepath, index=False)\n",
    "\n",
    "def load_datasets(*filepaths):\n",
    "    \"\"\"Load datasets using the `datasets` library.\"\"\"\n",
    "    dataset_files = {name: path for name, path in zip(['train', 'validation'], filepaths)}\n",
    "    return load_dataset(\"csv\", data_files=dataset_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb9e6656-80bb-4fe2-9067-aab61a925bb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "PROCESSED_DATA_PATH = \"../data/processed\"\n",
    "\n",
    "save_datasets([df_train, df_test, df_val],\n",
    "          os.path.join(PROCESSED_DATA_PATH, \"train.csv\"),\n",
    "          os.path.join(PROCESSED_DATA_PATH, \"test.csv\"),\n",
    "          os.path.join(PROCESSED_DATA_PATH, \"val.csv\"))\n",
    "\n",
    "\n",
    "dataset = load_datasets(os.path.join(PROCESSED_DATA_PATH, \"train.csv\"), os.path.join(PROCESSED_DATA_PATH, \"val.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "670cf691-4574-4c95-9d60-a6c0dcebec1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea9094f8-d992-4249-a68c-9c6dd5a00620",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = [label for label in dataset['train'].features.keys() if label not in ['index', 'text']]\n",
    "id2label = {idx: label for idx, label in enumerate(labels)}\n",
    "label2id = {label: idx for idx, label in enumerate(labels)}\n",
    "\n",
    "print(labels, id2label, label2id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0581d2eb-48a5-407e-8877-86aad77c7c50",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(dataset, tokenizer, labels):\n",
    "    \"\"\"Preprocess the data for training.\"\"\"\n",
    "    def preprocess_batch(examples):\n",
    "        text = examples[\"text\"]\n",
    "        encoding = tokenizer(text, padding=\"max_length\", truncation=True, max_length=512)\n",
    "\n",
    "        labels_batch = {k: examples[k] for k in examples.keys() if k in labels}\n",
    "        labels_matrix = np.zeros((len(text), len(labels)))\n",
    "        for idx, label in enumerate(labels):\n",
    "            labels_matrix[:, idx] = labels_batch[label]\n",
    "\n",
    "        encoding[\"labels\"] = labels_matrix.tolist()\n",
    "        return encoding\n",
    "\n",
    "    return dataset.map(preprocess_batch, batched=True, remove_columns=dataset['train'].column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e52a07c5-014e-40ab-a348-a6e8005cae2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "encoded_dataset_dict = preprocess_data(dataset, tokenizer, labels)\n",
    "\n",
    "encoded_dataset_dict.set_format(\"torch\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16d931bb-5435-4e96-96d7-20b015ad0557",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46c571d6-c908-4de7-bdf3-e5ef86977fc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSequenceClassification\n",
    "from transformers import TrainingArguments, Trainer\n",
    "from sklearn.metrics import f1_score, roc_auc_score, accuracy_score\n",
    "from transformers import EvalPrediction\n",
    "from transformers import EarlyStoppingCallback\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64b99356-63c6-4e80-8869-c1369d3aa808",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelForSequenceClassification.from_pretrained(\"bert-base-uncased\",\n",
    "                                                           num_labels=len(labels),\n",
    "                                                           id2label=id2label,\n",
    "                                                           label2id=label2id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac3918b1-c5fa-4a89-917f-41d5e59f1776",
   "metadata": {},
   "outputs": [],
   "source": [
    "def multi_label_metrics(predictions, labels, threshold=0.5):\n",
    "    # first, apply sigmoid on predictions which are of shape (batch_size, num_labels)\n",
    "    sigmoid = torch.nn.Sigmoid()\n",
    "    probs = sigmoid(torch.Tensor(predictions))\n",
    "    # next, use threshold to turn them into integer predictions\n",
    "    y_pred = np.zeros(probs.shape)\n",
    "    y_pred[np.where(probs >= threshold)] = 1\n",
    "    # finally, compute metrics\n",
    "    y_true = labels\n",
    "    f1_micro_average = f1_score(y_true=y_true, y_pred=y_pred, average=\"micro\")\n",
    "    roc_auc = roc_auc_score(y_true, y_pred, average=\"micro\")\n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "    # return as dictionary\n",
    "    metrics = {\"f1\": f1_micro_average,\n",
    "               \"roc_auc\": roc_auc,\n",
    "               \"accuracy\": accuracy}\n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc924da8-ba58-4cac-a010-83478ebc4cc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(p: EvalPrediction):\n",
    "    predictions = p.predictions[0] if isinstance(p.predictions, tuple) else p.predictions\n",
    "    result = multi_label_metrics(\n",
    "        predictions=predictions,\n",
    "        labels=p.label_ids)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "697e10d0-a05a-42ae-8c41-0a67b95b7890",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_arguments = TrainingArguments(\n",
    "    f\"../models/bert-fine-tuned-nlp-disaster-tweets-v1\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=64,\n",
    "    per_device_eval_batch_size=64,\n",
    "    num_train_epochs=1,\n",
    "    weight_decay=0.01,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"accuracy\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc9ad6fa-e3c1-443d-a008-61f624519a3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model,\n",
    "    training_arguments,\n",
    "    train_dataset=encoded_dataset_dict[\"train\"],\n",
    "    eval_dataset=encoded_dataset_dict[\"validation\"],\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics,\n",
    "    callbacks=[EarlyStoppingCallback(early_stopping_patience=5)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c57660e7-98a1-41e1-a8f5-d960fb19f751",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c61a0368-961e-43ba-b42a-a9b191f755ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.evaluate()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
